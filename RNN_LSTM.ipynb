{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import re\n",
    "from string import ascii_letters\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Flatten,Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from WCPre import PreSentence2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "file_path = \"./weights/w2c_LSTM-L20-L10-dim50-e{epoch:02d}-loss{loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv file \n",
    "File = 'Data/train.csv'\n",
    "dataSet = pd.read_csv(File)\n",
    "del dataSet['id']\n",
    "X = dataSet['comment_text']\n",
    "X = X.values\n",
    "Y = dataSet.T.drop('comment_text').T\n",
    "Y = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[1:100]\n",
    "# Y = Y[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(X)):\n",
    "    # Remove \\\\n Convert I'm --> I am etc \\n\",\n",
    "    comment = X[i]\n",
    "    comment = PreSentence2Vec.preProcessing(comment)\n",
    "    # Remove the link & numbers \\n\",\n",
    "    comment = PreSentence2Vec.preNumberLink(comment)\n",
    "    # Remove the punctuation\\n\",\n",
    "    comment = PreSentence2Vec.prePunctuation(comment)\n",
    "    # Captital --> lowercase --> wordlist \\n\",\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(\"[^\\w]\", \" \",  comment).split()\n",
    "    # Remove stopwords\\n\",\n",
    "    comment = [word for word in comment if word not in stopwords.words('english')]\n",
    "    # Remove Non-English\"\n",
    "    character = set(ascii_letters)\n",
    "    comment = [word for word in comment if any(letter in character for letter in word)]\n",
    "    \n",
    "    X[i] = ' '.join(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish preprocessing\n"
     ]
    }
   ],
   "source": [
    "print(\"finish preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-77b64b4ea804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Word Embedding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Find %s in training sets\" % len(word_index))\n",
    "\n",
    "maxLength = max(max(X, key=len))\n",
    "X = pad_sequences(X, maxlen=maxLength)\n",
    "\n",
    "X_train = X[0:round(len(X)*0.9)]\n",
    "X_validate = X[round(len(X)*0.9):]\n",
    "Y_train = Y[0:round(len(Y)*0.9)]\n",
    "Y_validate = Y[round(len(Y)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n1696\n"
     ]
    }
   ],
   "source": [
    "# Splite it into Training & Testing data\n",
    "\n",
    "# load word embeddings for the embedding layer\n",
    "embeddingDIM = 50\n",
    "embeddingsRoute = \"./Embedding/glove.twitter.%s.txt\" % embeddingDIM\n",
    "embeddings_index = {}\n",
    "f = open(embeddingsRoute, 'rt', encoding='utf8')\n",
    "print(\"Load embedding matrix\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embeddingDIM))\n",
    "\n",
    "# Word --> Vector\n",
    "count = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count += 1 \n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 1345, 50)          90700     \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 1345, 24)          7200      \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 1345, 12)          1776      \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 16140)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 96846     \n=================================================================\nTotal params: 196,522\nTrainable params: 105,822\nNon-trainable params: 90,700\n_________________________________________________________________\nNone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89 samples, validate on 10 samples\nEpoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e01-loss0.6670.hdf5\n\r89/89 [==============================] - 14s 160ms/step - loss: 0.6670 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e02-loss0.6499.hdf5\n\r89/89 [==============================] - 11s 129ms/step - loss: 0.6499 - acc: 0.4719 - val_loss: 0.0000e+00 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e03-loss0.6252.hdf5\n\r89/89 [==============================] - 11s 129ms/step - loss: 0.6252 - acc: 0.8090 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e04-loss0.5953.hdf5\n\r89/89 [==============================] - 12s 130ms/step - loss: 0.5953 - acc: 0.9775 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e05-loss0.5652.hdf5\n\r89/89 [==============================] - 12s 132ms/step - loss: 0.5652 - acc: 0.9775 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e06-loss0.5372.hdf5\n\r89/89 [==============================] - 12s 129ms/step - loss: 0.5372 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e07-loss0.5229.hdf5\n\r89/89 [==============================] - 11s 127ms/step - loss: 0.5229 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e08-loss0.5180.hdf5\n\r89/89 [==============================] - 11s 128ms/step - loss: 0.5180 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e09-loss0.5317.hdf5\n\r89/89 [==============================] - 12s 132ms/step - loss: 0.5317 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: saving model to ./weights/w2c_LSTM-L20-L10-dim50-e10-loss0.5380.hdf5\n\r89/89 [==============================] - 12s 140ms/step - loss: 0.5380 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(len(word_index) + 1, embeddingDIM, weights=[embedding_matrix], input_length=maxLength, trainable=False))\n",
    "model.add(LSTM(24, dropout=0.01, recurrent_dropout=0.01, use_bias=True,\n",
    "                   return_sequences=True))\n",
    "model.add(LSTM(12, dropout=0.01, recurrent_dropout=0.01, use_bias=True,\n",
    "                   return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(Y.shape[1],activation='softmax', use_bias=True))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [\n",
    "    # Metrics(),\n",
    "    # LearningRateScheduler(step_decay),\n",
    "    # EarlyStopping(monitor=\"val_acc\", patience=5, verbose=1),\n",
    "    # ReduceLROnPlateau(monitor=monitor_value, factor=0.2, patience=2, min_lr=0.001),\n",
    "    ModelCheckpoint(file_path, monitor='val_loss', save_best_only=False, period=1, verbose=1),\n",
    "]\n",
    "model.fit(X_train, Y_train, validation_data=(X_validate, Y_validate), batch_size=128,\n",
    "              initial_epoch=0, epochs=100, shuffle=False,\n",
    "              verbose=1, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}